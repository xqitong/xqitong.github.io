<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[second-hand-house-spider]]></title>
    <url>%2F2018%2F05%2F16%2Fsecond-hand-house-spider%2F</url>
    <content type="text"><![CDATA[第一个爬虫程序，爬了链家网的二手房信息。 作为刚入门的小白，最好的学习方式就是拿一个项目练手，所以就想到了搜集一下二手房信息。主要是因为房子信息比较敏感，因为屌丝们不是在当房奴，就是在当房奴的路上，要么就是当不起房奴。。。好了，不扯淡了，三天时间，边看《Python网络爬虫与信息提取》的视频，边学用typora写了个学习笔记，边敲了代码。传统行业从业，不是专业程序员，只是在当程序员的路上，大神轻拍。。。 链家网url结构分析：获得城市子域名代码，进入http://&lt;city&gt;.lianjia.com，例如sh-上海，bj-北京 以小区为爬取目标： 获得该城市的小区栏目页面，http://&lt;city&gt;.lianjia.com/xiaoqu/pg&lt;n&gt;/ 获得该城市的各区县页面，http://&lt;city&gt;.lianjia.com/xiaoqu/&lt;district&gt;/pg&lt;n&gt;/ 获得该区县的各板块页面，http://&lt;city&gt;.lianjia.com/xiaoqu/&lt;area&gt;/pg&lt;n&gt;/ 以行政区为爬取目标： 获得该城市的二手房栏目页面，https://&lt;city&gt;.lianjia.com/ershoufang/pg&lt;n&gt;/ 获得该城市某区二手房栏目页面，https://&lt;city&gt;.lianjia.com/ershoufang/&lt;district&gt;/pg&lt;n&gt;/ 这次仅仅用来爬取二手房信息并生成关注度的热力图，由于链家每个栏目最多显示100页信息，为了更多的了解相关城区的二手房信息，按行政区爬取。 以宇宙第一首都北京市为例：https://bj.lianjia.com/ershoufang/&lt;district&gt;/ &lt;district&gt;：为北京的行政区，为了方便爬取，建一个行政区字典： 1bjdistricts =&#123;'东城区':'dongcheng','西城区':'xicheng','朝阳区':'chaoyang','海淀区':'haidian','丰台区':'fengtai','石景山区':'shijingshan','通州区':'tongzhou','昌平区':'changping','大兴区':'daxing','亦庄开发区':'yizhuangkaifaqu','顺义区':'shunyi','房山区':'fangshan','门头沟区':'mentougou','平谷区':'pinggu','怀柔区':'huairou','密云区':'miyun','延庆区':'yanqing','燕郊':'yanjiao','香河':'xianghe'&#125; pg&lt;n&gt;:为页数，依据相关行政区获取，没有抓取这个信息，点开网站看一眼就可以，东城与西城不到100，海淀和朝阳这样的大区都超过了100页，所以最大是100。 具体实现： 使用requests来访问链家网页面信息 使用beautifulsoup和lxml库解析页面信息 使用re库提取信息，包括： [&#39;小区编号&#39;,&#39;房屋编号&#39;,&#39;小区名&#39;,&#39;户型&#39;,&#39;面积(m2)&#39;,&#39;朝向&#39;,&#39;装修&#39;,&#39;楼层&#39;,&#39;年代&#39;,&#39;区位&#39;,&#39;总价(万)&#39;,&#39;均价(元/m2)&#39;,&#39;关注(人)&#39;,&#39;带看(次)&#39;,&#39;坐标&#39;] 使用百度地图API获取坐标信息（需提前申请API调用的key，填入ak处） 将信息存入CSV以备后续分析（不会数据库。。😭） 百度地图（没学过网页的相关知识，就是边学边弄）： 用了百度的模板，添加了行政区域边界，和不同风格的模板，就是玩玩，还挺有意思。 遇到的问题 坐标反馈错误，一开始只用小区信息提交给baidu，发现反馈的数据很多出了北京市，还有返回广东的坐标，后面将提教信息完善，补充了成北京 朝阳区 双井 富力城这样的格式，返回的坐标基本没有问题，但是也有部分跨区的。 正则提取房屋信息时，户型以为都时x室x厅，后面发现还有x房间x卫这样的格式，这些问题基本就是遇到了再解决。 热力图生成用了关注人数和带看人数的加权平均，发现关注人数很多，但是带看很少，我认为关注人数更加能反应关注度，这个信息是购房者主动释放的，而带看是链家工作人员录入的。 由于没有考虑太多异常处理，不调用百度API时，基本没出过什么错，但是调用API时偶尔会出错，查了一下，也不是API哪里的问题，在提取页面后面加了个print()想看输出了什么，就没出错了，没搞清楚… 生成的热力图：调整了一下地图样式 最后来一张四区的集合图，北京真（tm）大： 关于信息的提取还是很有必要的，比如，我之前一直在西二环某传统行业工作，西二环房价那是杠杠的贵，那时一直感觉西南二环好偏僻，直到17年有一天，西南二环某盘从五万，半年直接翻了一倍，才发现还是自己太无知….最后还是那句：侠之大者，为国接盘。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>requests</tag>
        <tag>bs4</tag>
        <tag>re</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WAV格式音频学习]]></title>
    <url>%2F2018%2F05%2F14%2FWAV%E6%A0%BC%E5%BC%8F%E9%9F%B3%E9%A2%91%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[以前的整理的一篇文章。 周末没事，在家用pyaudio捣鼓了一下wav的读入，播放，与频谱分析。 正常人听觉的频率范围大约在20Hz~20kHz之间。为了保证声音不失真，采样频率应该在40kHz左右。常用的音频采样频率有8kHz、11.025kHz、22.05kHz、16kHz、37.8kHz、44.1kHz、48kHz等，如果采用更高的采样频率，还可以达到DVD的音质。 先上两个链接： 比特率：https://baike.baidu.com/item/%E6%AF%94%E7%89%B9%E7%8E%87/1022775?fr=aladdin PCM编码：https://baike.baidu.com/item/pcm%E7%BC%96%E7%A0%81/10865033?fr=aladdin 摘一些出来学习： 音频 800 bps – 能够分辨的语音所需最低码率（需使用专用的FS-1015语音编解码器） 8 kbps —电话质量（使用语音编码） 8-500 kbps –Ogg Vorbis和MPEG1 Player1/2/3中使用的有损音频模式 500 kbps–1.4 Mbps —44.1KHz的无损音频，解码器为FLAC Audio,WavPack或Monkey’s Audio 1411.2 - 2822.4 Kbps —脉冲编码调制(PCM)声音格式CD光碟的数字音频 5644.8 kbps —SACD使用的Direct Stream Digital格式 码率计算公式:基本的算法是：【码率】（kbps)=【文件大小】（字节）X8/【时间】（秒）*1000音频文件专用算法：【比特率】（kbps)=【量化采样点】（kHz）×【位深】（bit/采样点）×【声道数量】（一般为2） 结论： 1.一个采样率为44.1KHz，采样大小为16bit，双声道的PCM编码的WAV文件， 比特率= 44.1K×16×2 =1411.2 Kbps。 我们常见的Audio CD就采用了PCM编码，一张光盘的容量只能容纳72分钟的音乐信息，而常见的wav歌曲大部分是PCM格式的。 2.数据了计算：我找了首刘惜君-我很快乐 数据量=（采样频率×采样位数bit×声道数×时间）/8=[44.1×1000×16×2×（3×60+33）] /（8×1024×1024） =37573200B约35.83MB 算出来的大小与歌曲也一样，大小：35.8 MB (37,643,974 字节)；占用空间：35.9 MB (37,650,432 字节) 3.一帧PCM是：2048次采样组成的（网上搜到的），但是我自己用数据算了下，感觉不对，先写下面，有懂的帮指导一下： 采样率framerate：44100Hz帧数nframes:9410940时长time:213s 数据量：37573200Byte 帧数nframes/时长time=9410940/213=44182帧/秒，约等与采样率，怎么回事，我算错了么。。。汗 如果按2048次计算，一帧时间播放时间 = 2048 * 1000000/44100= 46.4ms，一帧的解码时间须控制在46.4ms内。 4.PCM格式 PCM(Pulse Code Modulation)也被称为 脉码编码调制。PCM中的声音数据没有被压缩，如果是单声道的文件，采样数据按时间的先后顺序依次存入。(它的基本组织单位是BYTE(8bit)或WORD(16bit)) 样本大小 数据格式 最小值 最大值 8位PCM unsigned int 0 225 16位PCM int -32767 32767 4.文件格式 5.代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596import wave import numpy as np from pyaudio import PyAudio import pylab import matplotlib.pyplot as plt #定义数据流块 chunk = 2048 #只读方式打开wav文件 f = wave.open("that.wav","rb") #创建PyAudio对象: p = PyAudio() #打开数据流 stream = p.open(format = p.get_format_from_width(f.getsampwidth()), channels = f.getnchannels(), rate = f.getframerate(), output = True) #读取数据 # _wave_params(nchannels=2, sampwidth=2, framerate=44100, nframes=9410940, comptype='NONE', compname='not compressed') #(声道数，采样精度，采样率，帧数，......) params = f.getparams() print(params) nchannels,sampwidth,framerate,nframes = params[0],params[1],params[2],params[3] #读取完整的帧数据到datawav中，这是一个string类型的数据 datawav = f.readframes(nframes) #将f关掉 f.close() # with open("t.txt","w") as ftxt: # # ftxt.writelines(str(datawav[:10000])) # # ftxt.close() #将波形数据转换为数组 # A new 1-D array initialized from raw binary or text data in a string. datause = np.fromstring(datawav, dtype=np.short) #将wave_data数组改为2列，行数自动匹配。在修改shape的属性时，需使得数组的总长度不变。 datause.shape = -1, 2 #将数组转置，分别得到两个声道的序列 datause = datause.T #time 得到每帧的绝对时间，也是一个数组，datause[0]datause[1]配对形成系列点坐标 #音频帧的播放时长 = 一个帧对应的采样点个数 / 采样频率(单位为s) # 则，当前一帧的播放时间 = 2048* 1000000/44100= 46.4ms time = np.arange(0, nframes) * (1.0/framerate) ##* chunk #采样点数，修改采样点数和起始位置进行不同位置和长度的音频波形分析 #根据采样定理知采样频率要大于信号频率2倍，所以这里设置采样频率为44100赫兹 N = 44100 #每秒采样次数 start = 100 * N #开始采样位置 nsamp = 10 * N #采样区间时间长度 wavedata = datause[0][start:start+nsamp] x=np.linspace(0,1,nsamp) yy = np.fft.fft(wavedata) yf = abs(yy) #取绝对值 yf1 = yf/nsamp #归一化处理 yf2 = yf1[range(int(nsamp//2))] #由于对称性，只取一半区间 fre = np.arange(len(yy)) fre2 = fre[range(int(len(x)//2))] #取一半区间 ##Original wave plt.subplot(221) plt.plot(x[0:nsamp//50],wavedata[0:nsamp//50]) plt.title('Original wave') ##绘制频谱图 plt.subplot(222) plt.plot(fre,yf,'r') plt.title('FFT of Mixed wave(two sides frequency range)',fontsize=7,color='#7A378B') #归一化 plt.subplot(223) plt.plot(fre,yf1,'g') plt.title('FFT of Mixed wave(normalization)',fontsize=9,color='r') #一半 plt.subplot(224) plt.plot(fre2,yf2,'b') plt.title('FFT of Mixed wave',fontsize=10,color='#F08080') plt.show() stream.write(datawav[4*start:4*start+4*nsamp])##length datawav 37643760;length datause 18821880;length wavedata 441000 ##注意，四个字节为一个采样点 # plt.title("Night.wav's Frames") # plt.subplot(211) # plt.plot(time, datause[0],color = 'green') # plt.subplot(212) # plt.plot(time, datause[1]) # plt.show() #播放 # while True: # data = f.readframes(chunk) # stream.write(data) # print(data) # if data == b'': # break #停止数据流 stream.stop_stream() stream.close() #关闭 PyAudio p.terminate() 我最后验证了时间ok，start相当于从100s开始播放了10s，上面输出的与直接用播放器放出来是同一段 。 6.频谱分析图 7.歌曲地址 https://pan.baidu.com/s/1hmeXmfUxQVYfBd0EGKGCYQ 参考： https://www.cnblogs.com/lzxwalex/p/6922099.html https://www.cnblogs.com/lidabo/p/3729615.html https://www.cnblogs.com/lzxwalex/p/6922099.html http://blog.sina.com.cn/s/blog_40793e970102w3m2.html https://blog.csdn.net/pi9nc/article/details/12570841]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>音频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[itertools模块总结]]></title>
    <url>%2F2018%2F05%2F14%2Fitertools%E6%A8%A1%E5%9D%97%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[itertools模块总结python官方library中有一个itertools模块，官方是这么介绍的： 123itertools — Functions creating iterators for efficient loopingThe module standardizes a core set of fast, memory efficient tools that are useful by themselves or in combination. Together, they form an “iterator algebra” making it possible to construct specialized tools succinctly and efficiently in pure Python. 所以，itertools模块是一个创建快速、高效迭代器的模块。 分类： 无限型（Infinite Iterators） 终止于最短输入序列的迭代器（Iterators terminating on the shortest input sequence） 组合生成器 （Combinatoric generators） 无限迭代器count(start, step) 起始参数(start)默认值为0 步长(step)默认值为1 作用: 返回以start开头的均匀间隔step步长的值 1234from itertools import countfor item in count(1,10): if item &gt; 150: break print(item,end=" ") 结果打印输出：1 11 21 31 41 51 61 71 81 91 101 111 121 131 141 如果没有截止条件，理论上会一直打印下去。 cycle(iterable) iterable 为可迭代对象 作用:保存迭代对象的每个元素的副本，无限的重复返回每个元素的副本 123from itertools import cyclefor item in cycle('abcdef'): print(item,end=" ") 结果打印输出：a b c d e f a b c d e…… repeat(elem[, times]) elem为述字符串，数字等对象 times为迭代次数，默认为无限次 作用:按照指定的迭代次数times重复返回elem，如果不指定times，则为无限次 123from itertools import repeatfor item in repeat('abcdef',4): print(item,end=" ") 结果打印输出：abcdef abcdef abcdef abcdef 终止于最短输入序列的迭代器accumulate(seq[,func]) seq为可迭代对象 func为累积执行的函数(默认为operator.add) 1234from itertools import accumulatefrom operator import mulfor item in accumulate([1,2,3,4,5],mul): print(item,end=" ") 结果打印输出：1 2 6 24 120 函数源码，划重点，理解迭代器和生成器的用法： 12345678910111213def accumulate(iterable, func=operator.add): '''Return running totals''' # accumulate([1,2,3,4,5]) --&gt; 1 3 6 10 15 # accumulate([1,2,3,4,5], operator.mul) --&gt; 1 2 6 24 120 it = iter(iterable) try: total = next(it) except StopIteration: return yield total for element in it: total = func(total, element) yield total chain(*iterables) *iterables为一个或多个可迭代序列 作用:返回所有可迭代序列 123from itertools import chainfor item in chain('abcdef','123'): print(item,end=" ") 结果打印输出：a b c d e f 1 2 3 compress(data, selectors) data为数据对象 selectors为选择器(规则) 作用:返回数据对象中对应规则为True的元素 123from itertools import compressfor item in compress('abcdef',[1,0,1,1,0,0]): print(item,end=" ") 结果打印输出：a c d dropwhile(pred,seq) pred为判断谓词 seq为可迭代序列 作用：当pred为false时开始迭代 123from itertools import dropwhilefor item in dropwhile(lambda x: x&lt;5,[1,4,6,4,1]): print(item,end=" ") 结果打印输出：6 4 1 takewhile(pred,seq) pred为判断谓词 seq为可迭代对象 作用：当pred为false时停止迭代 123from itertools import takewhilefor item in takewhile(lambda x: x&lt;5,[1,4,6,4,1]): print(item,end=" ") 结果打印输出：1 4 filterfalse(pred,seq) pred为判断谓词 seq为可迭代对象 作用：当pred为false时迭代 1234#与filter相反from itertools import filterfalsefor item in filterfalse(lambda x: x % 2,range(10)): print(item,end=" ") 结果打印输出：0 2 4 6 8 groupby(iterable[, key]) iterable为可迭代对象 key分组的键值 作用：创建一个迭代器，对iterable生成的连续项进行分组，在分组过程中会查找重复项。如果iterable在多次连续迭代中生成了同一项，则会定义一个组，如果将此函数应用一个分类列表，那么分组将定义该列表中的所有唯一项，key（如果已提供）是一个函数，应用于每一项，如果此函数存在返回值，该值将用于后续项而不是该项本身进行比较，此函数返回的迭代器生成元素(key, group)，其中key是分组的键值，group是迭代器，生成组成该组的所有项。 注意：当每次key函数返回的值变化时就产生一个新分组或break，所以需要先用key函数对对象进行排序。 1234567891011121314from itertools import groupbydef keyfunc(h): if h &gt; 180: return 'tall' elif h &lt; 160: return 'short' else: return 'middle'data = [190,180,182,160,155,170,167,178]data = sorted(data,key=keyfunc)for k,g in groupby(data,key=keyfunc): print(k) print(list(g)) 结果打印输出： 123456middle[180, 160, 170, 167, 178]short[155]tall[190, 182] 12345from itertools import islice,groupbyfor k, g in groupby('AAAABBBCCDAABBB'): print(k,list(g))# [k for k, g in groupby('AAAABBBCCDAABBB')] --&gt; A B C D A B# [list(g) for k, g in groupby('AAAABBBCCD')] --&gt; AAAA BBB CC D 结果打印输出： 123456A [&apos;A&apos;, &apos;A&apos;, &apos;A&apos;, &apos;A&apos;]B [&apos;B&apos;, &apos;B&apos;, &apos;B&apos;]C [&apos;C&apos;, &apos;C&apos;]D [&apos;D&apos;]A [&apos;A&apos;, &apos;A&apos;]B [&apos;B&apos;, &apos;B&apos;, &apos;B&apos;] islice(iterable, stop) / islice(iterable, start, stop[, step ]) iterable为可迭代对象 start、stop、step分别为起始位置、终止位置、和步长 作用：创建一个迭代器，生成项的方式类似于切片返回值： iterable[start : stop : step]，将跳过前start个项，迭代在stop所指定的位置停止，step指定用于跳过项的步幅。与切片不同，负值不会用于任何start，stop和step，如果省略了start，迭代将从0开始，如果省略了step，步幅将采用1. 1234567from itertools import islicefor i in islice('ABCDEFG', 2): print(i) # islice('ABCDEFG', 2) --&gt; A B# islice('ABCDEFG', 2, 4) --&gt; C D# islice('ABCDEFG', 2, None) --&gt; C D E F G# islice('ABCDEFG', 0, None, 2) --&gt; A C E G starmap(func, seq) func：计算函数 seq：需传递的参数 作用：创建一个迭代器，生成值func(*item),其中item来自iterable，只有当iterable生成的项适用于这种调用函数的方式时，此函数才有效。 注意：与map()与starmap()的区别类似与function(a,b)和function(*c) 1234from itertools import starmapfor i in starmap(pow,[(2,3),(3,2),(4,3)]): print(i,end=" ")#等价的map写法list(map(lambda x,y:pow(x,y),a,b)) 结果打印输出：8 9 64 tee(iterable, n=2) iterable为可迭代对象 n为需要的个数 作用：从iterable创建n个独立的迭代器，创建的迭代器以n元组的形式返回，n的默认值为2，此函数适用于任何可迭代的对象，但是，为了克隆原始迭代器，生成的项会被缓存，并在所有新创建的迭代器中使用，一定要注意，不要在调用tee()之后使用原始迭代器iterable，否则缓存机制可能无法正确工作。 123from itertools import teefor ntee in tee("123456789",3): print(list(ntee)) 结果打印输出： 123[&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;, &apos;5&apos;, &apos;6&apos;, &apos;7&apos;, &apos;8&apos;, &apos;9&apos;][&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;, &apos;5&apos;, &apos;6&apos;, &apos;7&apos;, &apos;8&apos;, &apos;9&apos;][&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;, &apos;5&apos;, &apos;6&apos;, &apos;7&apos;, &apos;8&apos;, &apos;9&apos;] zip_longest(iter1,iter2, …) iter1,iter2等为可迭代对象 作用：与zip()相同，但是迭代过程会持续到所有输入迭代变量iter1,iter2等都耗尽为止，如果没有使用fillvalue关键字参数指定不同的值，则使用None来填充已经使用的迭代变量的值。 12from itertools import zip_longestprint(list(zip_longest('abcd','12',fillvalue='-'))) 结果打印输出：[(‘a’, ‘1’), (‘b’, ‘2’), (‘c’, ‘-‘), (‘d’, ‘-‘)] 组合生成器product(iter1,iter2, … [repeat=1]) iter1,iter2等为可迭代对象 repeat是一个关键字参数，指定重复生成序列的次数 作用：创建一个迭代器，生成表示item1，item2等中的项目的笛卡尔积的元组 1234from itertools import productfor ele in product('abcd','12'): print(ele,end=' ')# product(range(2), repeat=3) --&gt; 000 001 010 011 100 101 110 111 结果打印输出：(‘a’, ‘1’) (‘a’, ‘2’) (‘b’, ‘1’) (‘b’, ‘2’) (‘c’, ‘1’) (‘c’, ‘2’) (‘d’, ‘1’) (‘d’, ‘2’) permutations(iterable [,r]): iterable 为可迭代对象 r为长度 作用：创建一个迭代器，返回iterable中所有长度为r的项目序列，如果省略了r，那么序列的长度与iterable中的项目数量相同 12345from itertools import permutationsfor ele in permutations('abcd',r=2): print(ele,end=' ')# permutations(range(3)) --&gt; 012 021 102 120 201 210#The number of items returned is n! / (n-r)! when 0 &lt;= r &lt;= n or zero when r &gt; n. 结果打印输出：(‘a’, ‘b’) (‘a’, ‘c’) (‘a’, ‘d’) (‘b’, ‘a’) (‘b’, ‘c’) (‘b’, ‘d’) (‘c’, ‘a’) (‘c’, ‘b’) (‘c’, ‘d’) (‘d’, ‘a’) (‘d’, ‘b’) (‘d’, ‘c’) combinations(iterable, r) iterable 为可迭代对象 r为长度 作用：创建一个迭代器，返回iterable中所有长度为r的子序列，返回的子序列中的项按输入iterable中的顺序排序，元素不能重复出现 1234from itertools import combinationsfor ele in combinations('abcd',r=2): print(ele,end=' ')#The number of items returned is n! / r! / (n-r)! when 0 &lt;= r &lt;= n or zero when r &gt; n. 结果打印输出：(‘a’, ‘b’) (‘a’, ‘c’) (‘a’, ‘d’) (‘b’, ‘c’) (‘b’, ‘d’) (‘c’, ‘d’) combinations_with_replacement(iterable, r) iterable 为可迭代对象 r为长度 作用：创建一个迭代器，返回iterable中所有长度为r的子序列，返回的子序列中的项按输入iterable中的顺序排序，元素可以重复出现 1234from itertools import combinations_with_replacementfor ele in combinations_with_replacement('abcd',r=2): print(ele,end=' ')#The number of items returned is (n+r-1)! / r! / (n-1)! when n &gt; 0. 结果打印输出：(‘a’, ‘a’) (‘a’, ‘b’) (‘a’, ‘c’) (‘a’, ‘d’) (‘b’, ‘b’) (‘b’, ‘c’) (‘b’, ‘d’) (‘c’, ‘c’) (‘c’, ‘d’) (‘d’, ‘d’)]]></content>
      <categories>
        <category>python模块</category>
      </categories>
      <tags>
        <tag>itertools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[date类和datetime类]]></title>
    <url>%2F2018%2F05%2F14%2Fdate%E7%B1%BB%E5%92%8Cdatetime%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[整理：QT 边学习边整理，内容来源于互联网，如有疑问请QQ 604615850联系，侵删。 前言在开发工作中，我们经常需要用到日期与时间，如： 作为日志信息的内容输出 计算某个功能的执行时间 用日期命名一个日志文件的名称 记录或展示某文章的发布或修改时间 其他 Python中提供了多个用于对日期和时间进行操作的内置模块：time模块、datetime模块和calendar模块。其中time模块是通过调用C库实现的，所以有些方法在某些平台上可能无法调用，但是其提供的大部分接口与C标准库time.h基本一致。time模块相比，datetime模块提供的接口更直观、易用，功能也更加强大。 一、相关术语的解释 UTC time Coordinated Universal Time，世界协调时，又称 格林尼治天文时间（GMT）、世界标准时间。与UTC time对应的是各个时区的local time，东N区的时间比UTC时间早N个小时，因此UTC time + N小时 即为东N区的本地时间；而西N区时间比UTC时间晚N个小时，即 UTC time - N小时 即为西N区的本地时间； 中国在东8区，因此比UTC时间早8小时，可以以UTC+8进行表示。 epoch time 表示时间开始的起点；它是一个特定的时间，不同平台上这个时间点的值不太相同，对于Unix而言，epoch time为 1970-01-01 00:00:00 UTC。 timestamp（时间戳） 也称为Unix时间 或 POSIX时间；它是一种时间表示方式，表示从格林尼治时间1970年1月1日0时0分0秒开始到现在所经过的毫秒数，其值为float类型。 但是有些编程语言的相关方法返回的是秒数（Python就是这样），这个需要看方法的文档说明。需要说明的是时间戳是个差值，其值与时区无关。 二、时间的表现形式常见的时间表示形式为： 时间戳 格式化的时间字符串 Python中还有其它的时间表示形式： time模块的time.struct_time datetime模块的datetime类 这里简单说下time.struct_time，包含如下属性： 下标/索引 属性名称 描述 0 tm_year 年份，如 2017 1 tm_mon 月份，取值范围为[1, 12] 2 tm_mday 一个月中的第几天，取值范围为[1-31] 3 tm_hour 小时， 取值范围为[0-23] 4 tm_min 分钟，取值范围为[0, 59] 5 tm_sec 秒，取值范围为[0, 61] 6 tm_wday 一个星期中的第几天，取值范围为[0-6]，0表示星期一 7 tm_yday 一年中的第几天，取值范围为[1, 366] 8 tm_isdst 是否为夏令时，可取值为：0 , 1 或 -1 属性值的获取方式有两种： 可以把它当做一种特殊的有序不可变序列通过 下标/索引 获取各个元素的值，如t[0] 也可以通过 .属性名 的方式来获取各个元素的值，如t.tm_year。 需要说明的是struct_time实例的各个属性都是只读的，不可修改。 三、time模块time模块主要用于时间访问和转换，这个模块提供了各种与时间相关的函数。 1. 函数列表 方法/属性 描述 time.altzone 返回与utc时间的时间差，以秒为单位（西区该值为正，东区该值为负）。其表示的是本地DST 时区的偏移量，只有daylight非0时才使用。 time.clock() 返回当前进程所消耗的处理器运行时间秒数（不包括sleep时间），值为小数；该方法Python3.3改成了time.process_time() time.asctime([t]) 将一个tuple或struct_time形式的时间（可以通过gmtime()和localtime()方法获取）转换为一个24个字符的时间字符串，格式为: “Fri Aug 19 11:14:16 2016”。如果参数t未提供，则取localtime()的返回值作为参数。 time.ctime([secs]) 功能同上，将一个秒数时间戳表示的时间转换为一个表示当前本地时间的字符串。如果参数secs没有提供或值为None，则取time()方法的返回值作为默认值。ctime(secs)等价于asctime(localtime(secs)) time.time() 返回时间戳（自1970-1-1 0:00:00 至今所经历的秒数） time.localtime([secs]) 返回以指定时间戳对应的本地时间的 struct_time对象格式（可以通过下标，也可以通过 .属性名 的方式来引用内部属性） time.localtime(time.time() + n*3600) 返回n个小时后本地时间的 struct_time对象格式（可以用来实现类似crontab的功能） time.gmtime([secs]) 返回指定时间戳对应的utc时间的 struct_time对象格式 time.gmtime(time.time() + n*3600) 返回n个小时后utc时间的 struct_time对象（可以通过 .属性名 的方式来引用内部属性）格式 time.strptime(time_str, time_format_str) 将时间字符串转换为struct_time时间对象，如：time.strptime(&#39;2017-01-13 17:07&#39;, &#39;%Y-%m-%d %H:%M&#39;) time.mktime(struct_time_instance) 将struct_time对象实例转换成时间戳 time.strftime(time_format_str, struct_time_instance) 将struct_time对象实例转换成字符串 获取时间戳格式的时间 12&gt;&gt;&gt; time.time()1486188022.862 获取struct_time格式的时间 1234&gt;&gt;&gt; time.localtime()time.struct_time(tm_year=2018, tm_mon=4, tm_mday=29, tm_hour=16, tm_min=7, tm_sec=36, tm_wday=6, tm_yday=119, tm_isdst=0)&gt;&gt;&gt; time.gmtime()#返回指定时间戳对应的utc时间的 struct_time对象格式time.struct_time(tm_year=2018, tm_mon=4, tm_mday=29, tm_hour=8, tm_min=8, tm_sec=37, tm_wday=6, tm_yday=119, tm_isdst=0) 获取字符串格式的时间 1234&gt;&gt;&gt; time.ctime()'Sun Apr 29 16:10:07 2018'&gt;&gt;&gt; time.asctime()'Sun Apr 29 16:10:13 2018' 时间戳格式转struct_time格式时间 123456789&gt;&gt;&gt; t1 = time.time()&gt;&gt;&gt; print(t1)1524989702.20803&gt;&gt;&gt; t2 = time.localtime(t1)&gt;&gt;&gt; print(t2)time.struct_time(tm_year=2018, tm_mon=4, tm_mday=29, tm_hour=16, tm_min=15, tm_sec=2, tm_wday=6, tm_yday=119, tm_isdst=0)&gt;&gt;&gt; t3 = time.gmtime(t1)#返回指定时间戳对应的utc时间的 struct_time对象格式&gt;&gt;&gt; print(t3)time.struct_time(tm_year=2018, tm_mon=4, tm_mday=29, tm_hour=8, tm_min=15, tm_sec=2, tm_wday=6, tm_yday=119, tm_isdst=0) 2. 时间格式转换时间戳格式的时间 与 字符串格式的时间 虽然可以通过ctime([secs])方法进行转换，但是字符串格式不太适应中国国情。因此，整体而言，它们 不能直接进行转换，需要通过struct_time作为中介，转换关系如下： 说明：上面的 ‘%H:%M:%S’ 可以直接用 ‘%X’ 代替。 四、 datetime模块datetime模块提供了处理日期和时间的类，既有简单的方式，又有复杂的方式。它虽然支持日期和时间算法，但其实现的重点是为输出格式化和操作提供高效的属性提取功能。 1. datetime模块中定义的类datetime模块定义了以下几个类： 类名称 描述 datetime.date 表示日期，常用的属性有：year, month和day datetime.time 表示时间，常用属性有：hour, minute, second, microsecond datetime.datetime 表示日期时间 datetime.timedelta 表示两个date、time、datetime实例之间的时间间隔，分辨率（最小单位）可达到微秒 datetime.tzinfo 时区相关信息对象的抽象基类。它们由datetime和time类使用，以提供自定义时间的而调整。 datetime.timezone Python 3.2中新增的功能，实现tzinfo抽象基类的类，表示与UTC的固定偏移量 需要说明的是：这些类的对象都是不可变的。 类之间的关系： 1234567objecttimedeltetzinfotimezonetimedatedatetime 2. datetime模块中定义的常量 常量名称 描述 datetime.MINYEAR datetime.date或datetime.datetime对象所允许的年份的最小值，值为1 datetime.MAXYEAR datetime.date或datetime.datetime对象所允许的年份的最大值，只为9999 3. datetime.date类datetime.date类的定义： 1date(year, month, day) --&gt; date object year, month 和 day都是是必须参数，各参数的取值范围为： 参数名称 取值范围 year [MINYEAR, MAXYEAR] month [1, 12] day [1, 指定年份的月份中的天数] 类方法和属性 类方法/属性名称 描述 date.max date对象所能表示的最大日期：9999-12-31 date.min date对象所能表示的最小日志：00001-01-01 date.resoluation date对象表示的日期的最小单位：天 date.today() 返回一个表示当前本地日期的date对象 date.fromtimestamp(timestamp) 根据跟定的时间戳，返回一个date对象 12345678910111213&gt;&gt;&gt; import time&gt;&gt;&gt; from datetime import date&gt;&gt;&gt; &gt;&gt;&gt; date.maxdatetime.date(9999, 12, 31)&gt;&gt;&gt; date.mindatetime.date(1, 1, 1)&gt;&gt;&gt; date.resolutiondatetime.timedelta(1)&gt;&gt;&gt; date.today()datetime.date(2018, 4, 29)&gt;&gt;&gt; date.fromtimestamp(time.time())datetime.date(2018, 4, 29) 对象方法和属性 对象方法/属性名称 描述 d.year 年 d.month 月 d.day 日 d.replace(year[, month[, day]]) 生成并返回一个新的日期对象，原日期对象不变 d.timetuple() 返回日期对应的time.struct_time对象 d.toordinal() 返回日期是是自 0001-01-01 开始的第多少天 d.weekday() 返回日期是星期几，[0, 6]，0表示星期一 d.isoweekday() 返回日期是星期几，[1, 7], 1表示星期一 d.isocalendar() 返回一个元组，格式为：(year, week_number, isoweekday) d.isoformat() 返回‘YYYY-MM-DD&#39;格式的日期字符串 d.strftime(format) 返回指定格式的日期字符串，与time模块的strftime(format, struct_time)功能相同 1234567891011121314151617181920212223242526272829&gt;&gt;&gt; d = date.today()&gt;&gt;&gt; d.year2018&gt;&gt;&gt; d.month4&gt;&gt;&gt; d.day29&gt;&gt;&gt; d.replace(2015)datetime.date(2015, 4, 29)&gt;&gt;&gt; d.replace(2015,3,3)datetime.date(2015, 3, 3)&gt;&gt;&gt; d.timetuple()time.struct_time(tm_year=2018, tm_mon=4, tm_mday=29, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=6, tm_yday=119, tm_isdst=-1)&gt;&gt;&gt; d.toordinal()736813&gt;&gt;&gt; d.weekday()6&gt;&gt;&gt; d.isoweekday()7&gt;&gt;&gt; d.isocalendar()(2018, 17, 7)&gt;&gt;&gt; ddatetime.date(2018, 4, 29)&gt;&gt;&gt; d.isoformat()'2018-04-29'&gt;&gt;&gt; d.ctime()'Sun Apr 29 00:00:00 2018'&gt;&gt;&gt; d.strftime('%Y/%m/%d')'2018/04/29' 4. datetime.time类time类的定义: 1time([hour[, minute[, second[, microsecond[, tzinfo]]]]]) --&gt; a time object hour为必须参数，其他为可选参数。各参数的取值范围为： 参数名称 取值范围 hour [0, 23] minute [0, 59] second [0, 59] microsecond [0, 1000000] tzinfo tzinfo的子类对象，如timezone类的实例 类方法和属性 类方法/属性名称 描述 time.max time类所能表示的最大时间：time(23, 59, 59, 999999) time.min time类所能表示的最小时间：time(0, 0, 0, 0) time.resolution 时间的最小单位，即两个不同时间的最小差值：1微秒 1234567&gt;&gt;&gt; from datetime import time&gt;&gt;&gt; time.maxdatetime.time(23, 59, 59, 999999)&gt;&gt;&gt; time.mindatetime.time(0, 0)&gt;&gt;&gt; time.resolutiondatetime.timedelta(0, 0, 1) 对象方法和属性 对象方法/属性名称 描述 t.hour 时 t.minute 分 t.second 秒 t.microsecond 微秒 t.tzinfo 返回传递给time构造方法的tzinfo对象，如果该参数未给出，则返回None t.replace(hour[, minute[, second[, microsecond[, tzinfo]]]]) 生成并返回一个新的时间对象，原时间对象不变 t.isoformat() 返回一个‘HH:MM:SS.%f&#39;格式的时间字符串 t.strftime() 返回指定格式的时间字符串，与time模块的strftime(format, struct_time)功能相同 12345678910111213141516171819&gt;&gt;&gt; t=time(20,3,55,6789)&gt;&gt;&gt; t.hour20&gt;&gt;&gt; t.minute3&gt;&gt;&gt; t.second55&gt;&gt;&gt; t.microsecond6789&gt;&gt;&gt; print(t.tzinfo)None&gt;&gt;&gt; t.replace(03)datetime.time(3, 3, 55, 6789)&gt;&gt;&gt; t.isoformat()'20:03:55.006789'&gt;&gt;&gt; t.strftime('%H%M%S')'200355'&gt;&gt;&gt; t.strftime('%H%M%S.%f')'200355.006789' 5. datetime.datetime类datetime类的定义 1datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]]) year, month和day是必须要传递的参数，tzinfo可以是None或tzinfo子类的实例。 各参数的取值范围为： 参数名称 取值范围 year [MINYEAR, MAXYEAR] month [1, 12] day [1, 指定年份的月份中的天数] hour [0, 23] minute [0, 59] second [0, 59] microsecond [0, 1000000] tzinfo tzinfo的子类对象，如timezone类的实例 如果一个参数超出了这些范围，会引起ValueError异常。 类方法和属性 类方法/属性名称 描述 datetime.today() 返回一个表示当前本期日期时间的datetime对象 datetime.now([tz]) 返回指定时区日期时间的datetime对象，如果不指定tz参数则结果同上 datetime.utcnow() 返回当前utc日期时间的datetime对象 datetime.fromtimestamp(timestamp[, tz]) 根据指定的时间戳创建一个datetime对象 datetime.utcfromtimestamp(timestamp) 根据指定的时间戳创建一个datetime对象 datetime.combine(date, time) 把指定的date和time对象整合成一个datetime对象 datetime.strptime(date_str, format) 将时间字符串转换为datetime对象 12345678910111213141516&gt;&gt;&gt; from datetime import datetime, timezone, date&gt;&gt;&gt; datetime.today()datetime.datetime(2018, 4, 29, 17, 13, 40, 956245)&gt;&gt;&gt; datetime.now()datetime.datetime(2018, 4, 29, 17, 14, 16, 872156)&gt;&gt;&gt; datetime.utcnow()datetime.datetime(2018, 4, 29, 9, 14, 34, 280177)&gt;&gt;&gt; import time&gt;&gt;&gt; datetime.fromtimestamp(time.time())datetime.datetime(2018, 4, 29, 17, 15, 27, 266250)&gt;&gt;&gt; datetime.utcfromtimestamp(time.time())datetime.datetime(2018, 4, 29, 9, 16, 11, 24812)&gt;&gt;&gt; datetime.combine(datetime.date(2018,2,3),t)&gt;&gt;&gt; datetime.strptime('2017/02/04 20:49', '%Y/%m/%d %H:%M')datetime.datetime(2017, 2, 4, 20, 49) 对象方法和属性 对象方法/属性名称 描述 dt.year, dt.month, dt.day 年、月、日 dt.hour, dt.minute, dt.second 时、分、秒 dt.microsecond, dt.tzinfo 微秒、时区信息 dt.date() 获取datetime对象对应的date对象 dt.time() 获取datetime对象对应的time对象， tzinfo 为None dt.timetz() 获取datetime对象对应的time对象，tzinfo与datetime对象的tzinfo相同 dt.replace([year[, month[, day[, hour[, minute[, second[, microsecond[, tzinfo]]]]]]]]) 生成并返回一个新的datetime对象，如果所有参数都没有指定，则返回一个与原datetime对象相同的对象 dt.timetuple() 返回datetime对象对应的tuple（不包括tzinfo） dt.utctimetuple() 返回datetime对象对应的utc时间的tuple（不包括tzinfo） dt.toordinal() 同date对象 dt.weekday() 同date对象 dt.isocalendar() 同date独享 dt.isoformat([sep]) 返回一个‘%Y-%m-%d&#39; dt.ctime() 等价于time模块的time.ctime(time.mktime(d.timetuple())) dt.strftime(format) 返回指定格式的时间字符串 1234567891011121314151617181920212223242526272829303132333435363738&gt;&gt;&gt; from datetime import datetime, timezone&gt;&gt;&gt; dt=datetime.now()&gt;&gt;&gt; dtdatetime.datetime(2018, 4, 29, 17, 33, 26, 541752)&gt;&gt;&gt; dt.year2018&gt;&gt;&gt; dt.timestamp()1524994406.541752&gt;&gt;&gt; dt.date()datetime.date(2018, 4, 29)&gt;&gt;&gt; dt.time()datetime.time(17, 33, 26, 541752)&gt;&gt;&gt; dt.timetz()datetime.time(17, 33, 26, 541752)&gt;&gt;&gt; dt.replace(2017)datetime.datetime(2017, 4, 29, 17, 33, 26, 541752)&gt;&gt;&gt; dt.timetuple()time.struct_time(tm_year=2018, tm_mon=4, tm_mday=29, tm_hour=17, tm_min=33, tm_sec=26, tm_wday=6, tm_yday=119, tm_isdst=-1)&gt;&gt;&gt; dt.utctimetuple()time.struct_time(tm_year=2018, tm_mon=4, tm_mday=29, tm_hour=17, tm_min=33, tm_sec=26, tm_wday=6, tm_yday=119, tm_isdst=0)&gt;&gt;&gt; dt.toordinal()736813&gt;&gt;&gt; dt.weekday()6&gt;&gt;&gt; dt.isoweekday()7&gt;&gt;&gt; dt.isocalendar()(2018, 17, 7)&gt;&gt;&gt; dt.isoformat()'2018-04-29T17:33:26.541752'&gt;&gt;&gt; dt.isoformat('/')'2018-04-29/17:33:26.541752'&gt;&gt;&gt; dt.isoformat(' ')'2018-04-29 17:33:26.541752'&gt;&gt;&gt; dt.ctime()'Sun Apr 29 17:33:26 2018'&gt;&gt;&gt; dt.strftime('%Y%m%d %H:%M:%S.%f')'20180429 17:33:26.541752' 6. 使用datetime.datetime类对时间戳与时间字符串进行转换 7.datetime.timedelta类 timedelta对象表示连个不同时间之间的差值。如果使用time模块对时间进行算术运行，只能将字符串格式的时间 和 struct_time格式的时间对象 先转换为时间戳格式，然后对该时间戳加上或减去n秒，最后再转换回struct_time格式或字符串格式，这显然很不方便。而datetime模块提供的timedelta类可以让我们很方面的对datetime.date, datetime.time和datetime.datetime对象做算术运算，且两个时间之间的差值单位也更加容易控制。 这个差值的单位可以是：天、秒、微秒、毫秒、分钟、小时、周。 datetime.timedelta类的定义: 1class datetime.timedelta(days=0, seconds=0, microseconds=0, milliseconds=0, hours=0, weeks=0) 所有参数都是默认参数，因此都是可选参数。参数的值可以是整数或浮点数，也可以是正数或负数。内部值存储days、seconds 和 microseconds，其他所有参数都将被转换成这3个单位： 1毫秒转换为1000微秒 1分钟转换为60秒 1小时转换为3600秒 1周转换为7天 然后对这3个值进行标准化，使得它们的表示是唯一的： microseconds : [0, 999999] seconds : [0, 86399] days : [-999999999, 999999999] 类属性 类属性名称 描述 timedelta.min timedelta(-999999999) timedelta.max timedelta(days=999999999, hours=23, minutes=59, seconds=59, microseconds=999999) timedelta.resolution timedelta(microseconds=1) 实例方法和属性 实例方法/属性名称 描述 td.days 天 [-999999999, 999999999] td.seconds 秒 [0, 86399] td.microseconds 微秒 [0, 999999] td.total_seconds() 时间差中包含的总秒数，等价于: td / timedelta(seconds=1) 12345678910111213141516&gt;&gt;&gt; import datetime&gt;&gt;&gt; datetime.timedelta(365).total_seconds()31536000.0&gt;&gt;&gt; dt=datetime.datetime.now()&gt;&gt;&gt; dtdatetime.datetime(2018, 4, 29, 17, 47, 57, 449792)&gt;&gt;&gt; dt+datetime.timedelta(3)#三天后datetime.datetime(2018, 5, 2, 17, 47, 57, 449792)&gt;&gt;&gt; dt+datetime.timedelta(-3)datetime.datetime(2018, 4, 26, 17, 47, 57, 449792)&gt;&gt;&gt; dt+datetime.timedelta(hours=3)datetime.datetime(2018, 4, 29, 20, 47, 57, 449792)&gt;&gt;&gt; dt+datetime.timedelta(hours=-3)datetime.datetime(2018, 4, 29, 14, 47, 57, 449792)&gt;&gt;&gt; dt+datetime.timedelta(hours=3,seconds=30)datetime.datetime(2018, 4, 29, 20, 48, 27, 449792) 五、时间格式码time模块的struct_time以及datetime模块的datetime、date、time类都提供了strftime()方法，该方法是用来格式化一个日期、日期时间和时间，可以输出一个指定格式的时间字符串。相反strptime()函数就是从字符串表示的日期时间按格式化字符串要求转换为相应的日期时间。 对于time对象来说，格式化字符串不要使用年、月、日相关的字符，因为time对象没有相应的值。如果不幸使用了，只能默认输出为0值。 对于date对象来说，格式化字符串不要使用时、分、秒和微秒相关的字符，因为date对象没有相应的值。如果使用了，只能默认输出为0值。 由于strftime()函数是调用C语言lib库来实现的，所以在不同平台都支持，具体特定平台支持的细节，需要在平台上查看strftime文档说明。 下面列表符合C89和C99标准的格式化字符： 格式字符 意义 例子 注意事项 %a 星期几的英语缩写 Sun, Mon, …, Sat(en_US); So, Mo, …, Sa(de_DE) %A 星期几的英语全称 Sunday, Monday, …, Saturday(en_US) %w 星期几采用数字表示，0表示星期日，6表示星期六。 0，1，…，6 %d 用0补充的两位日期数字。 01，02，…，31 %b 月份采用缩写字符表示。 Jan, Feb,…, Dec(en_US) %B 月份采用全名称表示。 January, February, …,December(en_US) %m 月份采用0补充的两位数表示。 01，02，…，12 %y 年份采用0补充的两位数表示。 00，01，…，99 %Y 采用四位数表示的年份。 0001，0002，…，2013，2014，2015，…，9998，9999 %H 以0补充的24小时表示的小时。 00，01，…，23 %I 以0补充的12小时表示的小时。 00，01，…，12 %p 本地时间是上午还是下午。 AM，PM(en_US) %M 以0补充的分钟表示。 00，01，…，59 %S 以0补充的秒表示。 00，01，…，59 %f 以0补充的微秒表示。 000000，000001，…，999999 %z UTC偏移表示为+HHMM或-HHMM。 (empty)，+0000,-0400,+1030，… %Z 时区名称。 (empty)，UTC，EST，CST，… %j 以0补充的年的天数。 001，002，…，366 %U 一年里第几周，星期日作为一周开始。 00，01，…，53 %W 一年里第几周，星期一作为一周开始。 00，01，…，53 %c 采用本地合适日期和时间表示。 Tue Aug 16 21:30:00 1988(en_US) %x 采用本地合适日期表示。 08/16/88(None);08/16/1988(en_US) %% 输出百分号%。 % 六、pytz模块与tzlocal模块 pytz模块将时区信息数据库引入python，此信息库又称TZ database、Zoneinfo database，是一个主要应用于电脑程序以及操作系统的，可协作编辑世界时区信息的数据库。由于该数据库由David Olson创立，因而有些地方也将其称作Olson数据库。 tzlocal返回一个 包含当地的时区信息的tzinfo 对象（需要使用pytz）。 123456789101112131415161718192021222324252627282930313233343536&gt;&gt;&gt; import pytz&gt;&gt;&gt; from datetime import datetime, timedelta&gt;&gt;&gt; import time&gt;&gt;&gt; print(pytz.country_timezones("cn"))['Asia/Shanghai', 'Asia/Urumqi']&gt;&gt;&gt; nytz=pytz.timezone('America/New_York')&gt;&gt;&gt; shtz=pytz.timezone('Asia/Shanghai')&gt;&gt;&gt; fmt = '%Y-%m-%d %H:%M:%S %Z%z'&gt;&gt;&gt; datetime.datetime.now(nytz).strftime(fmt)'2018-04-29 20:48:07 EDT-0400'&gt;&gt;&gt; sh_time=datetime.datetime.now(shtz).strftime(fmt)'2018-04-30 08:48:08 CST+0800'#获取当前时区信息&gt;&gt;&gt; from tzlocal import get_localzone&gt;&gt;&gt; local_tz=get_localzone()&gt;&gt;&gt; local_tz&lt;DstTzInfo 'Asia/Shanghai' LMT+8:06:00 STD&gt;&gt;&gt;&gt; local_date = datetime.datetime.now(pytz.timezone(str(get_localzone())))&gt;&gt;&gt; local_datedatetime.datetime(2018, 4, 30, 9, 42, 29, 234674, tzinfo=&lt;DstTzInfo 'Asia/Shanghai' CST+8:00:00 STD&gt;)&gt;&gt;&gt; local_date = datetime.datetime.now()&gt;&gt;&gt; local_datedatetime.datetime(2018, 4, 30, 9, 42, 53, 241570)##&gt;&gt;&gt; eastern = pytz.timezone('US/Eastern')&gt;&gt;&gt; amsterdam = pytz.timezone('Europe/Amsterdam')&gt;&gt;&gt; type(eastern)&lt;class 'pytz.tzfile.US/Eastern'&gt;&gt;&gt;&gt; loc_dt = eastern.localize(datetime(2002, 10, 27, 6, 0, 0))&gt;&gt;&gt; type(loc_dt)&lt;class 'datetime.datetime'&gt;&gt;&gt;&gt; print(loc_dt.strftime(fmt))2002-10-27 06:00:00 EST-0500&gt;&gt;&gt; ams_dt = loc_dt.astimezone(amsterdam)&gt;&gt;&gt; ams_dt.strftime(fmt)'2002-10-27 12:00:00 CET+0100' 七、总结那么Python中处理时间时，使用time模块好，还是用datetime模块好呢？就我个人而言，datetime模块基本上可以满足需要，且用起来确实比较方便。对于time模块，我只是在取当前时间的时间戳时会用到time.time()方法，当然也可以通过datetime.datetime.now().timestamp()来获取，只是显得复杂一点。我觉得还是看个人习惯吧，没有什么绝对的好坏之分。]]></content>
      <categories>
        <category>python模块</category>
      </categories>
      <tags>
        <tag>datetime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python网络爬虫与信息提取]]></title>
    <url>%2F2018%2F05%2F14%2FPython%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[整理：QT 最近在学python，了解到了爬虫，下了北京理工大学嵩天老师的课程，顺便学了用Typora做笔记，还用第一种技术路线爬了链家的二手房信息，利用百度地图做了热力图，感觉还挺好玩。不过裸辞在家….转行之路还是挺艰辛。记笔记勉励自己，也可分享给学友共同进步，个人水平有限，如有错漏还请见谅，如果侵犯到任何人的权益，请告知。大部分内容来源于网络，如需沟通请下方微信或QQ：604615850联系，侵删。 如有人介绍工作，不胜感激。 正文开始 The website is API，掌握定向网络数据爬取和网页解析的基本能力： Requests：自动爬去HTML页面自动网络请求提交； robots.txt：网络爬虫排除标准； Beautiful Soup：解析HTML页面； Re：正则表达式详解提取页面关键信息； Scrapy：网络爬虫原理，专业爬虫框架； Request库与HTTP协议request库主要方法 方法 说明 requests.request() 构造一个请求，支撑以下个方法的基础方法 requests.get() 获取HTML网页的主要方法，对应于HTTP的GET requests.head() 获取HTML网页头信息的方法，对应于HTTP的HEAD requests.post() 向HTML网页提交POST请求的方法，对应于HTTP的POST requests.put() 向HTML网页提交PUT请求的方法，对应于HTTP的PUT requests.patch() 向HTML网页提交局部修改请求，对应于HTTP的PATCH requests.delete() 向HTML网页提交删除请求，对应于HTTP的DELETE 为了更好理解上面的方法，我们先了解一下HTTP协议。 HTTP协议HTTP（Hypertext Transfer Protocol）协议，即，超文本传输协议； 是一种基于”请求与响应“模式的、无状态(前后两次请求之间无关联)的应用层协议； 采用URL作为定位网络资源的标识：URL格式为http://host[:port][path]； host：合法的Internet主机域名或IP地址； port：端口号，缺省端口为80； path：请求资源的路径； URL是通过HTTP协议存取资源的INTERNET路径，一个URL对应一个数据资源。 HTTP协议对资源的操作（与requests对应方法对应）； 方法 说明 GET 请求获取URL位置的资源 HEAD 请求获取URL位置资源的响应消息报告，即获得该资源的头部信息 POST 请求获取URL位置的资源后附加新的数据 PUT 请求向URL位置存储一个资源，覆盖原URL位置的资源 PATCH 请求局部更新URL位置的资源，即改变该处资源的部分内容 DELETE 请求删除URL位置存储的资源 response对象的属性 属性 说明 r.starus_code HTTP请求的返回状态，200表示连接成功，404表示失败 r.text HTTP响应内容的字符串形式，即，URL对应的页面内容 r.encoding 从HTTP header中猜测的响应内容编码方式 r.apparent_encoding 从内容中分析出的响应内容编码方式（备选编码方式） r.content HTTP响应内容的二进制形式 123456789101112131415&gt;&gt;&gt; import requests&gt;&gt;&gt; mysite='http://www.baidu.com' &gt;&gt;&gt; r = requests.get(mysite)&gt;&gt;&gt; print(r.status_code)200&gt;&gt;&gt; print(type(r))&lt;class 'requests.models.Response'&gt;&gt;&gt;&gt; print(r.headers)&#123;'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/', 'Date': 'Mon, 30 Apr 2018 09:45:10 GMT', 'Content-Type': 'text/html', 'Content-Encoding': 'gzip', 'Last-Modified': 'Mon, 23 Jan 2017 13:28:12 GMT', 'Pragma': 'no-cache', 'Keep-Alive': 'timeout=38', 'Transfer-Encoding': 'chunked', 'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform'&#125;&gt;&gt;&gt; print(r.encoding)ISO-8859-1&gt;&gt;&gt; print(r.apparent_encoding)utf-8&gt;&gt;&gt; print(r.content)b'&lt;!DOCTYPE html&gt;\r\n&lt;!--STATUS OK--&gt;&lt;html&gt; 。。。。&lt;/body&gt; &lt;/html&gt;\r\n' r.encoding：如果header中不存在charset字段，则认为编码为ISO-8859-1；r.apparent_encoding：根据网页内容分析出的编码方式； 同时包含requests的全部信息： 12&gt;&gt;&gt; print(r.request.headers)&#123;'User-Agent': 'python-requests/2.9.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'&#125; requests.request()详解此方法为最基本的方法，可以构造一个请求，支撑其它各方法的基础方法，格式如下： requests.request(method,url,**kwargs) method：请求方式，对应get/put/post等7种； url：拟获取页面的url链接； kwargs：控制访问参数，共13个，均为可选项，如下： params：字典或字节序列，作为参数增加到url中； data：字典，字节序列或文件对象，作为Request的内容； json：JSON格式的数据，作为Request的内容； headers：字典，HTTP定制头（模拟浏览器进行访问）； cookies：字典或CpplieJar,Request中的cookie； auth：元组，支持HTTP认证功能； files：字典类型，传输文件； timeout：设定超时时间，秒为单位； proxies：字典类型，设定访问代理服务器，可以增加登陆认证； allow_redirects：True//False，默认为True，重定向开关； stream：True/False,默认为True,获取内容立即下载开关； verify：True/False,默认为True，认证SSL证书开关； cert：本地SSL证书路径。 12345&gt;&gt;&gt; import requests&gt;&gt;&gt; kv=&#123;'key1':'value','key2':'value2'&#125;&gt;&gt;&gt; r=requests.request('GET','http://python123.io/ws',params=kv)&gt;&gt;&gt; print(r.url)https://python123.io/ws?key1=value&amp;key2=value2 robots协议Robots Exclusion Standard 网络爬虫排除标准 作用：网站告知网络爬虫哪些页面可以抓取，哪些不行； 形式：在网站根目录下的robots.txt文件； 1234567891011121314151617案例：京东的Robots协议：https://www.jd.com/robots.txtUser-agent: * #所有的网络爬虫都定义为User-agent,并遵守如下协议：Disallow: /?* #不允许访问网络以？开头的路径Disallow: /pop/*.html #不允许访问Disallow: /pinpai/*.html?* #不允许访问User-agent: EtaoSpider #这个网络爬虫不允许爬去京东任何资源（以下三个一样）Disallow: / #根目录User-agent: HuihuiSpider #Disallow: / #User-agent: GwdangSpider #Disallow: / #User-agent: WochachaSpider #Disallow: /##Robots基本语法注释： *代表所有，/代表根目录# User-agent: *# Disallow: / # 如果没有Robots.txt文件即代表网站对访问没有限制 Robots的遵守方法 协议的使用： 网络爬虫：自动或人工识别robots.txt，再进行内容爬取； 约束性：Robots协议是建议但非约束性网络爬虫可以不遵守，但村子法律风险。 对Robots协议的理解： 爬取网页，玩转网页 爬取网站，爬取系列网站 爬取全网 访问量很小：可以遵守 非商业且偶尔：建议遵守 必须遵守 问量较大：必须遵守 商业利益：必须遵守 - 代码框架及异常处理12345678910111213#爬取网页的通用代码框架及异常处理import requestsdef getHTMLText(url): try: r = requests.get(url,timeout=30) r.raise_for_status() #如果r.starus_code不是200，产生一个requests.HTTPError r.encoding = r.apparent_encoding return r.text except: return "Errors occur"if __name__ == '__main__': url = 'http://www.baidu.com' print(getHTMLText(url)) 异常 说明 requests.RequestException There was an ambiguous exception that occurred while handling your request. requests.ConnectionError 网络连接错误异常，如DNS查询失败，拒绝连接等 requests.HTTPError HTTP错误异常 requests.URLRequired URL缺失异常 requests.TooManyRedirects 超过最大重定向次数，产生重定向错误 requests.ConnectTimeout 连接远程服务器超时异常 requests.Timeout 请求URL超时，产生超时异常 requests.ReadTimeout The server did not send any data in the allotted amount of time. 应用实例 访问亚马逊中国： 123456789101112131415161718import requestsdef getHTMLText(url): kv = &#123;'user-agent':'Mozilla/5.0'&#125;#网站过滤爬虫，模拟浏览器访问 try: r = requests.get(url,headers=kv) print(r.status_code,r.encoding,r.apparent_encoding) r.raise_for_status()#如果r.starus_code不是200，产生一个requests.HTTPError # r.encoding = r.apparent_encoding print(r.request.headers) return r.text except: return "Errors occur"if __name__ == '__main__': # url = 'https://item.jd.com/6946631.html' url='https://www.amazon.cn/dp/B07C2S9ZYM/ref=sr_1_1?s=wireless&amp;ie=UTF8&amp;qid=1525140296&amp;sr=1-1' print(getHTMLText(url)) #503 拒绝某些客户端的连接## 200 UTF-8 ISO-8859-2## &#123;'user-agent': 'Mozilla/5.0', 'Accept': '*/*', 'Connection': 'keep-alive', 'Accept-Encoding': 'gzip, deflate'&#125; 百度提交关键字 123456789101112import requeststry: kv=&#123;'wd':'Python'&#125; url='https://www.baidu.com/s' r = requests.get(url,params=kv) print(r.request.url) r.raise_for_status() print(len(r.text))except: print("Errors occur") # https://www.baidu.com/s?wd=Python# 227 网络图片的爬取与存储 1234567891011121314151617181920import requestsimport osurl = 'https://news.nationalgeographic.com/content/dam/news/2017/07/13/01-tardigrade.adapt.1190.1.jpg'root = './mypic/'#注：ubuntu中根目录”/“；当前目录“./”；当前目录的上一级目录(如果有上一级目录的话)“../”path = root + url.split('/')[-1].replace('.','_') + ".jpg"try: if not os.path.exists(root): os.mkdir(root) if not os.path.exists(path): r = requests.get(url) r.raise_for_status() with open(path,'wb') as f: f.write(r.content) f.close() print("Saved successfully") else: print("Already exited")except: print("Errors occur") IP地址归属地的自动查询 12345678910111213import requests#'http://www.ip138.com/ips138.asp?ip=202.204.80.112&amp;action=2'url = 'http://www.ip138.com/ips138.asp?'kv = &#123;'ip':'202.204.80.112','action':'2'&#125;try: r = requests.get(url, params=kv) #, headers=headkv r.raise_for_status()#如果r.starus_code不是200，产生一个requests.HTTPError r.encoding = r.apparent_encoding print(r.text[7250:7380]) print("Visit successfully") except: print("Errors occur") 网络爬虫引发的问题 引发的问题 爬虫骚扰，增加服务器负担； 爬虫的法律风险； 爬虫泄露隐私，突破简单的访问控制。 爬取网页，玩转网页 爬取网站，爬取系列网站 爬取全网 小规模，数据量小，爬取速度不敏感 中规模，数据规模较大，爬取速度敏感 大规模，搜索引擎爬取速度关键 requests库 scrapy库 定制开发 限制网络爬虫的技术手段： 来源审查：检查来访HTTP协议头的user-agent域，判断user-agent进行限制，只响应浏览器或友好爬虫的访问； 发布公告：通过robots.txt协议，告知所有爬虫，网站允许的爬取策略，要求爬虫遵守。 BeautifulSoup库BeautifulSoup 库及安装请求把数据返回来之后就要提取目标数据，不同的网站返回的内容通常有多种不同的格式， JSON：有类型的键值对key：nalue，适合程序处理；移动应用云端和节点的信息通信，无注释； XML：最早的通用信息标记语言，标签占用过多；Internet上的信息交互与传递； YAML：无类型的键值对，文本信息比例最高，可读性好；各类系统的配置文件，有注释易读； HTML文档从属与XML，现在就来讲讲如何从 HTML 中提取出感兴趣的数据，BeautifulSoup 是一个用于解析 HTML 文档的 Python 库，通过 BeautifulSoup，你只需要用很少的代码就可以提取出 HTML 中任何感兴趣的内容，此外，它还有一定的 HTML 容错能力，对于一个格式不完整的HTML 文档，它也可以正确处理。Beautiful Soup是python的一个库，最主要的功能是从网页抓取数据。 安装：pip install beautifulsoup4 文档：Beautiful Soup 4.2.0 文档 HTML 标签初识学习 BeautifulSoup4 前有必要先对 HTML 文档有一个基本认识，如下代码，HTML 是一个树形组织结构。 123456789&lt;html&gt; &lt;head&gt; &lt;title&gt;hello, world&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;BeautifulSoup&lt;/h1&gt; &lt;p&gt;如何使用BeautifulSoup&lt;/p&gt; &lt;body&gt;&lt;/html&gt; 通过预定义的&lt;&gt;...&lt;/&gt;标签形式组织不同类型的信息； 它由很多标签（Tag）组成，比如 html、head、title等等都是标签； 一个标签对构成一个节点，比如 … 是一个根节点； 节点之间存在某种关系，比如 h1 和 p 互为邻居，他们是相邻的兄弟（sibling）节点； h1 是 body 的直接子（children）节点，还是 html 的子孙（descendants）节点； body 是 p 的父（parent）节点，html 是 p 的祖辈（parents）节点； 嵌套在标签之间的字符串是该节点下的一个特殊子节点，比如 “hello, world” 也是一个节点，只不过没名字； 使用 BeautifulSoup构建一个 BeautifulSoup 对象需要两个参数，第一个参数是将要解析的 HTML 文本字符串，第二个参数告诉 BeautifulSoup使用哪个解析器来解析 HTML。解析器负责把 HTML 解析成相关的对象，而 BeautifulSoup 负责操作数据（增删改查）。”html.parser”是Python内置的解析器，”lxml”则是一个基于c语言开发的解析器，它的执行速度更快，不过它需要额外安装。 通过BeautifulSoup 对象就可以定位到 HTML 中的任何一个标签节点。 123456789101112131415161718192021&gt;&gt;&gt; from bs4 import BeautifulSoup &gt;&gt;&gt; text = """&lt;html&gt; &lt;head&gt; &lt;title &gt;hello, world&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;BeautifulSoup&lt;/h1&gt; &lt;p class="bold"&gt;如何使用BeautifulSoup&lt;/p&gt; &lt;p class="big" id="key1"&gt; 第二个p标签&lt;/p&gt; &lt;a href="http://www.fool.com" &gt;python&lt;/a&gt; &lt;/body&gt;&lt;/html&gt; """&gt;&gt;&gt; soup = BeautifulSoup(text, "html.parser")&gt;&gt;&gt; soup.title&lt;title&gt;hello, world&lt;/title&gt;&gt;&gt;&gt; soup.p&lt;p class="bold"&gt;如何使用BeautifulSoup&lt;/p&gt;&gt;&gt;&gt; soup.p.string'如何使用BeautifulSoup' BeatifulSoup 将 HTML 抽象成为 4 类基本元素，分别是Tag , Attributes，NavigableString, Comment 。每个标签节点就是一个Tag对象，NavigableString 对象一般是包裹在Tag对象中的字符串，BeautifulSoup 对象代表整个 HTML 文档。例如： 123456&gt;&gt;&gt; type(soup)&lt;class 'bs4.BeautifulSoup'&gt;&gt;&gt;&gt; type(soup.h1)&lt;class 'bs4.element.Tag'&gt;&gt;&gt;&gt; type(soup.p.string)&lt;class 'bs4.element.NavigableString'&gt; Tag每个 Tag 都有一个名字，它对应 HTML 的标签名称. 1234&gt;&gt;&gt; soup.h1.name'h1'&gt;&gt;&gt; soup.p.name'p' 标签还可以有属性，属性的访问方式和字典是类似的，它返回一个列表对象。 12&gt;&gt;&gt; soup.p['class']['bold'] NavigableString获取标签中的内容，直接使用 .stirng 即可获取，它是一个 NavigableString 对象，你可以显式地将它转换为 unicode字符串。 1234&gt;&gt;&gt; soup.p.string'如何使用BeautifulSoup'&gt;&gt;&gt; type(soup.p.string)&lt;class 'bs4.element.NavigableString'&gt; 数据提取如何从 HTML 中找到我们关心的数据？BeautifulSoup 提供了两种方式，一种是遍历，另一种是搜索，通常两者结合来完成查找任务。 遍历文档树 顾名思义，就是是从根节点 html 标签开始遍历，直到找到目标元素为止，遍历的一个缺陷是，如果你要找的内容在文档的末尾，那么它要遍历整个文档才能找到它，速度上就慢了，主要的遍历方向有三种： 下行遍历 属性 说明 .contents 子节点的列表，将&lt;tag&gt;所有儿子节点存入列表 .children 子节点的迭代类型，与.conents类似，用于循环遍历儿子节点 .descendants 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历 上行遍历 属性 说明 .parent 节点的父亲标签 .parents 节点先辈标签的迭代类型用于循环遍历先辈节点 平行遍历（同一个父亲节点下） 属性 说明 .next_sibling 返回按照HTML文本顺序的下一个平行节点标签 .previous_sibling 返回按照HTML文本顺序的上一个平行节点标签 .next_siblings 迭代类型，返回按照HTML文本顺序的后续所有平行节点标签 .previous_siblings 迭代类型，返回按照HTML文本顺序的前序所有平行节点标签 通过遍历文档树的方式获取标签节点可以直接通过.标签名的方式获取，内容也是一个节点，这里就可以用 .string的方式得到。遍历文档树的另一个缺点是只能获取到与之匹配的第一个子节点，例如，如果有两个相邻的 p 标签时，第二个标签就没法通过.p 的方式获取，这是需要借用 next_sibling 属性获取相邻且在后面的节点。此外，比如：.contents 获取所有子节点，.parent 获取父节点，例如： 1234567891011121314#获取body标签：&gt;&gt;&gt; soup.body&lt;body&gt;&lt;h1&gt;BeautifulSoup&lt;/h1&gt;&lt;p class="bold"&gt;如何使用BeautifulSoup&lt;/p&gt;&lt;p class="big" id="key1"&gt; 第二个p标签&lt;/p&gt;&lt;a href="http://foofish.net" rel="external nofollow"&gt;python&lt;/a&gt;&lt;/body&gt;#获取p标签&gt;&gt;&gt; soup.body.p&lt;p class="bold"&gt;如何使用BeautifulSoup&lt;/p&gt;#获取p标签的内容&gt;&gt;&gt; soup.body.p.string'如何使用BeautifulSoup' 搜索文档 搜索文档树是通过指定标签名来搜索元素，另外还可以通过指定标签的属性值来精确定位某个节点元素，最常用的两个方法就是 find 和 find_all。这两个方法在 BeatifulSoup 和 Tag 对象上都可以被调用。 find_all( name , attrs , recursive , string , **kwargs ) 返回一个列表类型，存储查找的结果； name：对标签名称的检索字符串； attrs：对标签名称属性值的检索字符串，可标注属性检索； recursive：是否对子孙全部检索，默认为True； string：&lt;&gt;...&lt;/&gt;中字符串区域的检索字符串； 简写形式&lt;tag&gt;(..)等价于&lt;tag&gt;.find_all(..) find_all 的返回值是一个 Tag 组成的列表，方法调用非常灵活，所有的参数都是可选的： 第一个参数 name 是标签节点的名字； 12&gt;&gt;&gt; soup.find_all("title")[&lt;title&gt;hello, world&lt;/title&gt;] 第二个参数是标签的class属性值； 123&gt;&gt;&gt; soup.find_all("p","big")#class_="big",因为class是Python关键字，所以这里指定为 class_ 。[&lt;p class="big" id="key1"&gt; 第二个p标签&lt;/p&gt;] kwargs 是标签的属性名值对，例如：查找有href属性值为 “http://www.fool.com&quot; 的标签； 12&gt;&gt;&gt; soup.find_all(href="http://www.fool.com")[&lt;a href="http://www.fool.com"&gt;python&lt;/a&gt;] 正则表达式； 123&gt;&gt;&gt; import re&gt;&gt;&gt; soup.find_all(href=re.compile("^http"))[&lt;a href="http://foofish.net"&gt;python&lt;/a&gt;] 布尔值（True/Flase），表示有属性或者没有该属性； 1234&gt;&gt;&gt; soup.find_all(id="key1")[&lt;p class="big" id="key1"&gt; 第二个p标签&lt;/p&gt;]&gt;&gt;&gt; soup.find_all(id=True)[&lt;p class="big" id="key1"&gt; 第二个p标签&lt;/p&gt;] 遍历和搜索相结合查找，先定位到 body 标签，缩小搜索范围，再从 body 中找 a 标签。 123&gt;&gt;&gt; body_tag = soup.body&gt;&gt;&gt; body_tag.find_all('a')[&lt;a href="http://www.fool.com"&gt;python&lt;/a&gt;] find( name , attrs , recursive , string , **kwargs ) find 方法跟 find_all 类似，唯一不同的地方是，它返回的单个 Tag 对象而非列表，如果没找到匹配的节点则返回 None。如果匹配多个 Tag，只返回第0个。 123456&gt;&gt;&gt; body_tag.find("a")&lt;a href="http://foofish.net"&gt;python&lt;/a&gt;&gt;&gt;&gt; body_tag.find("p")&lt;p class="bold"&gt;如何使用BeautifulSoup&lt;/p&gt;&gt;&gt;&gt; body_tag.find_all("p")[&lt;p class="bold"&gt;如何使用BeautifulSoup&lt;/p&gt;, &lt;p class="big" id="key1"&gt; 第二个p标签&lt;/p&gt;] get_text() 获取标签里面内容，除了可以使用.string 之外，还可以使用 get_text 方法，不同的地方在于前者返回的一个 NavigableString 对象，后者返回的是 unicode类型的字符串。但这个方法获取到tag中包含的所有文版内容包括子孙tag中的内容。 12345678910111213141516171819&gt;&gt;&gt; p1=body_tag.find('p').get_text()&gt;&gt;&gt; type(p1)&lt;class 'str'&gt;&gt;&gt;&gt; p1'如何使用BeautifulSoup'&gt;&gt;&gt; p2=body_tag.find('p').string&gt;&gt;&gt; type(p2)&lt;class 'bs4.element.NavigableString'&gt;&gt;&gt;&gt; p2'如何使用BeautifulSoup'#返回包含所有文版内容包括子酸tag中的内容&gt;&gt;&gt; soup.find('body').get_text()'\nBeautifulSoup\n如何使用BeautifulSoup\n 第二个p标签\npython\n'#通过参数指定tag的文本内容的分隔符&gt;&gt;&gt; soup.find('body').get_text("|")'\n|BeautifulSoup|\n|如何使用BeautifulSoup|\n| 第二个p标签|\n|python|\n'#去除获得文本内容的前后空白&gt;&gt;&gt; soup.find('body').get_text("|",strip=True)'BeautifulSoup|如何使用BeautifulSoup|第二个p标签|python' 实际场景中我们一般使用 get_text 方法获取标签中的内容。 扩展方法： 方法（参数同find_all()） 说明 &lt;&gt;.find() 搜索只返回一个结果，字符串类型 &lt;&gt;.find_parents() 在先辈节点中搜索，返回列表类型 &lt;&gt;.find_parent() 在先辈节点中返回一个结果，字符串类型 &lt;&gt;.find_next_siblings() 在后续平行节点中搜索，返回列表类型 &lt;&gt;.find_next_sibling() 在后续平行节点中返回一个结果，字符串类型 &lt;&gt;.find_previoust_siblings() 在前序平行节点中搜索，返回列表类型 &lt;&gt;.find_previoust_sibling() 在前序平行节点中返回一个结果，字符串类型 小结BeatifulSoup 是一个用于操作 HTML 文档的 Python 库，初始化 BeatifulSoup 时，需要指定 HTML 文档字符串和具体的解析器。BeatifulSoup 有3类常用的数据类型，分别是 Tag、NavigableString和 BeautifulSoup。查找 HTML元素有两种方式，分别是遍历文档树和搜索文档树，通常快速获取数据需要二者结合。 正则表达式库Re正则表达式：regular expression，是用来简洁表达一组字符串的表达式。 通用的字符串表达框架； 简洁表达一组字符串的表达式； 针对字符串表达的“简洁”和“特征”思想的工具； 判断某字符串的特征属性。 正则表达式在文本处理中十分有用： 表达文本类型的特征（病毒、入侵等）； 同时查找或替换一组字符串； 匹配字符串额全部或部分。 正则表达式的使用,需先编译，即将符合正则表达式语法的字符串转换成正则表达式特征。 正则表达式的语法正则表达式由字符和操作符构成。 常用操作符： 操作符 说明 实例 . 表示任何单个字符 a..b为所有以a头且以b尾的四字符串 [] 字符集，对单个字符给出取值范围 [a-z]表示a到z单个字符 [^] 非字符集，对单个字符给出排除范围 [^abc]表示非a或b或c的单个字符 * 前一个字符0此或无限次扩展 abc*表示ab，abc，abcc，abccc等 + 前一个字符1此或无限次扩展 abc+表示abc，abcc，abccc等 ? 前一个字符0次或1次拓展 abc?表示ab，abc &#124; 左右表达式任意一个 abc&#124;def表示abc或def {m} 扩展前一个字符m次 ab{2}c表示abbc {m,n} 扩展前一个字符m至n次（含n） ab{1，2}c表示abc，abbc ^ 匹配字符串开头 ^abc表示abc且在一个字符串的开头 $ 匹配字符串结尾 abc$表示abc且在一个字符串的结尾 () 分组标记，内部只能使用&#124;操作符 （abc&#124;def）表示abc或def \d(\D) 数字 等价于[0-9]([^0-9]) \w(\W) 单词字符 等价于[A-Za-z0-9]([^A-Za-z0-9]) \s(\S) 与所有空白字符匹配 等价于[ \t\v\n\f\r]([^ \t\v\n\f\r]) \b(\B) 单词边界，在单词边界位置匹配空串 \\b123\\b或r\b123\b 注释：正则表达式里的空格也作为常规字符，因此能与自己匹配。 经典正则表达式实例 ^[A-Za-z]+$：由26个字母组成的字符串 ^[A-Za-z0-9]+$：由26个字母和数字组成的字符串 ^[-+]?\d+$：整数形式的字符串 [1-9]\d{5}：中国境内邮政编码，6位 [\u4e00-\u9fa5]:匹配中文字符 re库的基本使用re库是python的标准库，主要用于字符串匹配。 12#直接导入库import re 正则表达式的表示类型 raw string类型（原生字符串类型）：不包含转义符的字符串； string类型，有转义字符； 常用功能函数 函数 说明 re.search() 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象 re.match() 从一个字符串的开始位置起（前缀）匹配正则表达式，返回match对象 re.findall() 搜索字符串，以列表类型返回全部能匹配的子串 re.split() 将一个字符串按照正则表达式匹配结果进行分割，返回列表类型 re.finditer() 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象 re.sub() 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串 match对象 主要属性： 属性 说明 .string 待匹配的文本 .re 匹配时使用的pattern对象（正则表达式） .pos 正则表达式搜索文本的开始的位置 .endpos 正则表达式搜索文本的结束位置 .group() 获得匹配后的字符串 .start() 匹配字符串再原始字符串的开始位置 .end() 匹配字符串再原始字符串的结束位置 .span() 返回(.start(),.end()) re.search(pattern,string,flags=0) pattern：正则表达式的字符串或原生字符串表示； string：待匹配字符串； flags：正则表达式使用时的控制标记； 常用标记 说明 re.I re.IGNORECASE 忽略正则表达式的大小写，[A-Z]能够匹配小写字符 re.M re.MULTILINE ^操作符能够将给定字符串的每行当做匹配开始 re.S re.DOTALL .操作符能够匹配所有字符，默认匹配处换行外的所有字符 1234import rematch=re.search('\\b[1-9]\d&#123;5&#125;\\b','BIT 100081')if match: print(match.group(0)) re.match(pattern,string,flags=0) 12345678import rematch=re.match('[1-9]\d&#123;5&#125;','BIT 100081')if not match: print('Return is None')else: print(match.groups())# match需要从头开始匹配，所以这个例子返回None，# 需要把字符调整'100081 BIT' re.findall(pattern,string,flags=0) 123import rels = re.findall('[1-9]\d&#123;5&#125;','BIT100081 TSU100084')print(ls) re.split(pattern,string,maxsplit=0,flags=0) maxsplit：最大分割数，剩余部分作为最后一个元素输出 12345678910import rels=re.split('[1-9]\d&#123;5&#125;','BIT100081 TSU100084')print(ls)#output['BIT', ' TSU', '']##ls=re.split('[1-9]\d&#123;5&#125;','BIT100081 TSU100084',maxsplit=1)print(ls)#output['BIT', ' TSU100084'] re.finditer(pattern,string,flags=0) 1234567#search只能找到第一个匹配结果，如果需要多个匹配用finditerimport refor m in re.finditer('[1-9]\d&#123;5&#125;','BIT100081 TSU100084'): print(m.group(0))#output100081100084 re.sub(pattern,repl，string,count=0，flags=0) repl：替换匹配字符串的字符串 count：匹配的最大替换次数 12345import res=re.sub('[1-9]\d&#123;5&#125;',':zipcode','BIT100081 TSU100084')print(s)#outputBIT:zipcode TSU:zipcode match.group()模式里的组 在正则表达式中一个重要的概念是组，使用圆括号括起来的模式段(...)，在考虑匹配时，它与被括起来的子模式匹配的串匹配，同时，圆括号还确定了一个被匹配的组。 在一次成功匹配中，模式串里的各个组也都成功匹配，与它们匹配成功的那一组字符串将从1开始编号，而后可以通过调用match.group(n)获取，match.groups()将得到这个从1开始的各个组匹配的串。作为特殊情况，组0就是与整个模式匹配的字符串，也可以通过match.group()来获取。 模式里各队圆括号确定的组按开括号的顺序编号，例如： 1234567import remat = re.search('.((.)e)f','abcdef')print(mat.group())print(mat.groups())#outputcdef('de', 'd') 组的另一个重要用途，再匹配中应用前面的成功匹配，建立前后的部分匹配之间的约束关系，例如： 123456789import remat = re.search('(.&#123;2&#125;) \\1','bbb cc aa aab')print(mat)print(mat.group())print(mat.groups())#output&lt;_sre.SRE_Match object; span=(7, 12), match='aa aa'&gt;aa aa('aa',) re库的另一种等价用法 函数式用法：一次性操作 1rst=re.search('[1-9]\d&#123;5&#125;','BIT 100081')； 面向对象法：编译后的多次操作 12pat=re.compile('[1-9]\d&#123;5&#125;')rst=pat.search('BIT 100081') re.compile(pattern,flags=0)，将正则表达式的字符串形式编译成正则表达式对象 pattern：正则表达式的字符串或原生字符串表示； flags：正则表达式使用时的控制标记； 1regex = re.compile('[1-9]\d&#123;5&#125;') 等价的方法调用： 函数 说明 regex.search() 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象 regex.match() 从一个字符串的开始位置起（前缀）匹配正则表达式，返回match对象 regex.findall() 搜索字符串，以列表类型返回全部能匹配的子串 regex.split() 将一个字符串按照正则表达式匹配结果进行分割，返回列表类型 regex.finditer() 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象 regex.sub() 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串 贪婪匹配和最小匹配12345match = re.search(r'PY.*N','PYANBNCNDN')#PYAN，PYANBN，PYANBNCN，PYANBNCNDNprint(match.group())#outputPYANBNCNDN python默认为贪婪匹配，即输出匹配长度最长的子串； 如何输出最小匹配？ 123456#在*后加?match = re.search(r'PY.*?N','PYANBNCNDN')#PYAN，PYANBN，PYANBNCN，PYANBNCNDNprint(match.group())#outputPYAN 最小匹配操作符： 操作符 说明 *? 前一个字符0次或无限次扩展，最小匹配 +? 前一个字符1次或无限次扩展，最小匹配 ?? 前一个字符0次或1次扩展，最小匹配 {m,n}? 扩展前一个字符m至n次（含n），最小匹配 应用实例(空)XPath的介绍与配置XPath是什么 是一门语言； 可以在XML文档中查找信息； 支持HTML； 通过元素和属性进行导航； 作用： 用来提取信息 比正则表达式厉害 比正则表达式简单 如何使用XPath 安装lxml库 from lxml import etree Selector = etree.HTML(网页源代码) Selector.xpath(表达式) XPath与HTML结构 树状结构 逐层展开 逐层定位 寻找独立节点 获取网页元素的XPath 手动分析法 Chrome生成法 应对XPath提取内容 //定位根节点 /往下层寻找 /text()提取文本内容 /@xxx提取属性内容 12345678910111213141516171819202122232425262728293031323334353637text = """&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;bookstore&gt;&lt;book category="COOKING"&gt; &lt;title lang="en"&gt; Everyday Italian&lt;/title&gt; &lt;author&gt;Giada De Laurentiis&lt;/author&gt; &lt;year&gt;2005&lt;/year&gt; &lt;price&gt;30.00&lt;/price&gt;&lt;/book&gt;&lt;book category="WEB"&gt; &lt;title lang="en"&gt;XQuery Kick Start&lt;/title&gt; &lt;author&gt;James McGovern&lt;/author&gt; &lt;author&gt;Per Bothner&lt;/author&gt; &lt;author&gt;Kurt Cagle&lt;/author&gt; &lt;author&gt;James Linn&lt;/author&gt; &lt;author&gt;Vaidyanathan Nagarajan&lt;/author&gt; &lt;year&gt;2003&lt;/year&gt; &lt;price&gt;49.99&lt;/price&gt;&lt;/book&gt;&lt;book category="WEB"&gt; &lt;title lang="en"&gt;Learning XML&lt;/title&gt; &lt;author&gt;Erik T. Ray&lt;/author&gt; &lt;year&gt;2003&lt;/year&gt; &lt;price&gt;39.95&lt;/price&gt;&lt;/book&gt;&lt;/bookstore&gt;"""from lxml import etreehtml = etree.HTML(text)&gt;&gt;&gt; html.xpath("//book[@category='COOKING']/title/text()")[' Everyday Italian']&gt;&gt;&gt; html.xpath("//book[@category='COOKING']/title/@lang")['en']&gt;&gt;&gt; html.xpath("//book[@category='WEB']/title/text()")['XQuery Kick Start', 'Learning XML']&gt;&gt;&gt; html.xpath("//book[@category='WEB']/title/@lang")['en', 'en'] XPath特殊用法： 包含字符串 contains(@属性名称，属性字符) 以相同的字符开头 start-with(@属性名称，属性字符相同部分) 标签套标签 string(.) Scrapy库Scrapy是一个三方库，需用pip install scrapy --user安装。但其并不是一个简单的函数功能库，而是一个爬虫框架。 爬虫框架是实现爬虫功能的一个软件结构和功能组合集合； 爬虫框架是一个半成品，能够帮助用户实现专业网络爬虫。 框架结构“5个模块+2个中间件”结构： 上图的数字代表数据的流向，解释如下 引擎从Spider 获取初始Request对象； 引擎将获取的Request对象交给调度器Scheduler，并向Spider要下一个Request对象； 调度器将下一个Request对象交给引擎； 引擎将Request对象交给下载器Downloader，途径下载器中间件； 网页下载完成，下载器Downloader生成一个Response对象， 并经过下载中间件交给引擎； 引擎收到Response对象， 并交给Spider处理， 途径 Spider Middleware； Spider 处理Response 对象， 并将提取的结构化数据构成Item，同时生成新的Request对象，一并交给引擎， 途径 Spider Middleware； 引擎将Item 交给Item Pipeline 处理， 将Request对象交给调度器Scheduler，并继续向Spider要Request对象，直到没有Request对象可处理； 从上面的结构图可看出， Scrapy 框架以Engine 为核心来运转，当调度器中没有Request需要爬取时，爬取任务结束。图中Engine、Downloader和Scheduler是已有实现，用户需要配置Spider和Item Piplines。其中Spider用来想整个框架提供URL链接，同时要解析从网络页面获得的内容；Item Piplines负责对提取的信息进行后处理。 用户需要或可能编写的模块如下： Downloader Middleware中间件： 实施ENGINE、DOWNLOADER和SCHEDULER之间进行用户可配置的控制； 功能：修改、丢弃、新增请求或响应（用户可以编写配置代码）； Spider： 解析Download返回的响应（Response）； 产生爬取项； 产生额外的爬取请求（Request） Item Piplines： 以流水线方式处理Spider产生的爬取项； 由一组操作顺序组成，类似流水线，每个操作是一个Item Pipline类型； 可能操作包括：清理、检验和查重爬取项中的HTML数据、将数据存储到数据库； Spider Middleware： 目的：对请求和爬取项的再处理； 功能：修改、丢弃、新增请求和爬取项； 使用步骤及数据类型Scrapy爬虫的使用步骤： 创建一个工程和Spider模板； 编写Spider； 编写Item Pipeline； 优化配置策略； Scrapy爬虫的数据类型： Request类 class scrapy.http.Request() Request对象表示一个HTTP请求； 由Spider生成，由Downloader执行； 常用的属性和方法： 属性或方法 说明 .url Request 对应的请求URL地址 .method 对应的请求方法，GET、POST等 .headers 字典类型风格的请求头 .body 请求内容主体，字符串类型 .meta 用户添加的扩展信息，在Scrapy内部模块间传递信息使用 .copy() 复制该请求 Response类 class scrapy.http.Reponse() Reponse对象表示一个HTTP响应； 由Downloader生成，由Spider处理； 常用的属性和方法： 属性或方法 说明 .url Response 对应的请求URL地址 .status HTTP状态码，默认是200 .headers Response对应的头部信息 .body Response对应的内容信息，字符串类型 .flags 一组标记 .request 产生Response类型对应的Request对象 .copy() 复制该请求 Item类 class scrapy.http.Item() Item对象表示一个从HTTP页面中提取的信息内容； 由Spider生成，由Item Pipeline处理； Item类似字典类型，可以按照字典类型操作； 信息提取方法 Srapy爬虫支持多种HTML信息提取方法： Beautiful Soup lxml re XPath Selector CSS Selector 其它的方法前面都有介绍，这里我们简单说下CSS Selector的基本使用： 12&lt;HTML&gt;.css('a::attr(href)').extract()#标签名称a，标签属性href Scrapy命令行Scrapy是为了持续运行设计的专业爬虫框架，提供操作的命令行。命令行格式如下： 1&gt;&gt;&gt; Scrapy &lt;command&gt; [optioins][args] Scrapy常用命令 命令 说明 格式 startproject 创建一个工程 scrapy startproject &lt;name&gt; [dir] genspider 创建一个爬虫 scrapy genspider [options] &lt;name&gt; &lt;domian&gt; settings 获得爬虫配置信息 scrapy settings [options] crwal 运行一个爬虫 scrapy crawl &lt;spider&gt; list 列出工程中所有的爬虫 scrapy list shell 启动URL调试命令行 scrapy shell [url] 为什么Scrapy采用命令行创建和运行爬虫？ 命令行（不是图像界面）更容易自动化，适合脚步控制； 本质上，Scrapy是给程序员用的，功能（不是图形界面）更重要。 Scrapy 与 Request比较 相同点： 两者都可以进行页面请求和爬取，python爬虫的两个重要技术路线； 两者可用性都好，文档丰富，入门简单； 两者都没有处理js，提交表单，应对验证码等功能（可扩展）； 不同点： request scrapy 页面及爬虫 网站级爬虫 功能库 框架 并发性考虑不足，性能较差 并发性好，性能较高 重点在于页面下载 重点在于爬虫结构 定制灵活 一般定制灵活，深度定制困难 上手十分简单 入门稍难 选用哪个技术路线开发爬虫 非常小的需求，request库； 不太小的需求，Scrapy库； 定制程度很高的需求（不考虑规模），自搭框架，request库 &gt; Scrapy库。 yield关键字yield生成器： 生成器是一个不断产生值的函数； 包含yield语句的函数是一个生成器； 生成器每次产生一个值（yield语句），函数被冻结，被唤醒后再产生一个值。 为何要用生成器： 更节省存储空间； 响应更迅速； 使用更加灵活。 实例1: 一个简单的demo演示HTML地址：http://python123.io/ws/demo.html 1). 生成工程文件scrapy startproject demo1 demo1/ ：外层目录 scrapy.cfg ：部署Scrapy爬虫的配置文件 demo1 Scrapy：框架的用户自定义python代码 __init__.py：初始化脚本 items.py：Items代码模板（继承类） middlewares.py：Middlewares代码模板（继承类） pipelines.py：Pipilines代码模板（继承类） settings.py：Scrapy爬虫的配置文件 spiders/：Spiders代码模板目录（继承类） __init__.py：初始化文件，无需修改 __pycache__/：缓存目录，无需修改 2). 在工程中产生一个Scrapy爬虫 1234------------------------------------------------------------~/myapp/scrapy_spider » cd demo1 ------------------------------------------------------------~/myapp/scrapy_spider/demo1 » scrapy genspider demo python123.io spiders/目录下生成了 demo.py： 123456789# -*- coding: utf-8 -*-import scrapyclass DemoSpider(scrapy.Spider): name = 'demo' allowed_domains = ['python123.io'] start_urls = ['http://python123.io/'] def parse(self, response): pass 其中，parse()用于处理响应，解析内容形成字典，发现新的URL爬取请求。 3). 配置产生的spider爬虫 123456789101112# -*- coding: utf-8 -*-import scrapyclass DemoSpider(scrapy.Spider): name = 'demo' #allowed_domains = ['python123.io'] start_urls = ['http://python123.io/ws/demo.html'] def parse(self, response): fname = response.url.split('/')[-1] with open(fname,'wb') as f: f.write(response.body) self.log('Saved file %s.' % name) 4). 运行爬虫获取网页 12&gt;&gt;&gt; scrapy crawl demo#页面保存与demo.html 实例2：股票信息爬取功能描述： 技术路线：scrapy； 目标：获取上交所和深交所所有股票的名称和交易信息； 输出：保存到文件中。 数据网站的确定： 获取股票列表：http://quote.eastmoney.com/stocklist.html 获取个股信息 百度股票：https://gupiao.baidu.com/stock/ 单个股票：https://gupiao.baidu.com/stock/sz002439.html 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#建立工程和Spider模板&gt;&gt;&gt; scrapy startproject BaiduStocks&gt;&gt;&gt; cd BaiduStocks&gt;&gt;&gt; scrapy genspider stocks baidu.com##spider# -*- coding: utf-8 -*-import scrapyimport reclass StocksSpider(scrapy.Spider): name = 'stocks' #allowed_domains = ['baidu.com'] start_urls = ['http://quote.eastmoney.com/stocklist.html'] def parse(self, response): for href in response.css('a::attr(href)').extract(): try: stock = re.findall(r'[s][hz]\d&#123;6&#125;',href)[0] url = 'https://gupiao.baidu.com/stock/' + stock + '.html' yield scrapy.Request(url,callback=self.parse_stock) except: continue def parse_stock(self, response): infoDict = &#123;&#125; stockInfo = response.css('.stock-bets') name = stockInfo.css('.bets-name').extract()[0] keyList = stockInfo.css('dt').extract() valueLIst = stockInfo.css('dd').extract() for i in range(len(keyList)): key = re.findall(r'&gt;.*&lt;/dt&gt;',keyList[i])[0][1:-5] try: val = re.findall(r'\d+\.?.*&lt;/dd&gt;',valueLIst[i])[0][0:-5] except: val = '--' infoDict[key] = val infoDict.update(&#123;'股票名称':re.findall('\s.*\(',name)[0].split()[0] + re.findall('\&gt;.*\&lt;',name)[0][1:-1]&#125;) yield infoDict##end spider##pipeline# -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.htmlclass BaidustocksPipeline(object): def process_item(self, item, spider): return itemclass BaidustocksInfoPipeline(object): def open_spider(self, spider): self.f = open('BaiduStcokInfo.txt','w') def close_spider(self,spider): self.f.close() def process_item(self,item,spider): try: line = str(dict(item)) + '\n' self.f.write(line) except: pass return item##end pipeline##settings# Configure item pipelines# See https://doc.scrapy.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; 'BaiduStocks.pipelines.BaidustocksInfoPipeline': 300,&#125;##end settings##运行&gt;&gt;&gt; scrapy crawl stocks 配置并发连接选项优化爬取速度： setting.py文件 选项 说明 CONCURRENT_REQUESTS Downloader最大并发请求下载数量，默认32 CONCURRENT_ITEMS Item Pipeline 最大并发ITEM处理数量，默认100 CONCURRENT_REQUESTS_PER_DOMAIN 每个目标域名最大的并发请求数量，默认8 CONCURRENT_REQUESTS_PER_IP 每个目标IP最大的并发请求数量，默认0，非0有效 Scrapy爬虫的地位 Python语言最好的爬虫框架； 具备企业级专业爬虫的扩展性（7*24高可靠性）； 千万级URL爬取管理与部署。 Scrapy爬虫足以支撑一般商业服务所需的爬虫能力。 普通价值： 基于linux服务器，7*24，稳定爬取输出； 商业级部署和应用（scrapyd-*）； 千万级URL爬取、内容分析和存储； 高阶价值： 基于docker，虚拟化部署； 中间件扩展，增加调度和监控； 各种反爬取对抗技术。 写在最后两种技术路线： requests-bs4-re scrapy(5+2结构) 由于这两种技术路线，还没法处理js的表单提交、爬取周期、入库管理等相关功能，还需要配置pyantomJS库来扩展来解析js。 对于https://pypi.python.org网站上以scrapy-开头的文件都是用来完善scrapy库，也可以用来尝试。 “君子曰：学不可以已。积土成山，风雨兴焉。”–荀子《劝学》 生命不息，学习不止。 感谢嵩老师的课程。]]></content>
      <categories>
        <category>python模块</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
</search>
